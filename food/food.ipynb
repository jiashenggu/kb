{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from allennlp.commands.evaluate import *\n",
    "from kb.include_all import *\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "archive_file = \"knowbert_wiki_wordnet_model\"\n",
    "cuda_device = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()  # 不加名称设置root logger\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 使用FileHandler输出到文件\n",
    "fh = logging.FileHandler('log.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 使用StreamHandler输出到屏幕\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# 添加两个Handler\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = load_archive(archive_file, cuda_device)\n",
    "config = archive.config\n",
    "prepare_environment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Params.from_file(\"/home/scruple/kb/knowbert_wiki_wordnet_model/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_params = config.pop('dataset_reader')\n",
    "# print(reader_params.as_dict())\n",
    "if reader_params['type'] == 'multitask_reader':\n",
    "    reader_params = reader_params['dataset_readers']['language_modeling']\n",
    "# reader_params['num_workers'] = 0\n",
    "validation_reader_params = {\n",
    "    \"type\": \"food\",\n",
    "    \"tokenizer_and_candidate_generator\": reader_params['base_reader']['tokenizer_and_candidate_generator'].as_dict()\n",
    "}\n",
    "dataset_reader = DatasetReader.from_params(Params(validation_reader_params))\n",
    "vocab = dataset_reader._tokenizer_and_candidate_generator.bert_tokenizer.vocab\n",
    "token2word = {}\n",
    "for k, v in vocab.items():\n",
    "    token2word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = dataset_reader.read(\"../data/food_compact.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instances[69][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = archive.model\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(\"start\")\n",
    "# metrics = evaluate(model, instances, iterator, cuda_device, \"\")\n",
    "data_iterator = DataIterator.from_params(Params(\n",
    "    {\"type\": \"basic\", \"batch_size\": 1}\n",
    "))\n",
    "data_iterator.index_with(model.vocab)\n",
    "iterator = data_iterator(instances,\n",
    "                            num_epochs=1,\n",
    "                            shuffle=False)\n",
    "logger.info(\"Iterating over dataset\")\n",
    "generator_tqdm = Tqdm.tqdm(iterator, total=data_iterator.get_num_batches(instances))\n",
    "with open('prompt2.txt', 'wt') as f:\n",
    "    for instance in generator_tqdm:\n",
    "        batch = nn_util.move_to_device(instance, cuda_device)\n",
    "        output_dict = model(**batch)\n",
    "        pooled_output = output_dict.get(\"pooled_output\")\n",
    "        contextual_embeddings = output_dict.get(\"contextual_embeddings\")\n",
    "        prediction_scores, seq_relationship_score = model.pretraining_heads(\n",
    "        contextual_embeddings, pooled_output\n",
    "        )\n",
    "        prediction_scores = prediction_scores.view(-1, prediction_scores.shape[-1])\n",
    "        # print(\"================\")\n",
    "        # print(prediction_scores)\n",
    "        # print(\"================\")\n",
    "        idx = torch.argmax(prediction_scores, dim = 1)\n",
    "        line = \"\"\n",
    "        for i in idx.cpu().numpy():\n",
    "            line += token2word[i] + \" \"\n",
    "        \n",
    "        print(line, file=f)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f24110ee2db4d48ba4b1000bf22589bea4791e16fd0364b7a9875ecc3c21267"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('knowbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
