{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, GPT2TokenizerFast\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49780/49780 [00:00<00:00, 143338.40it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/nas/home/gujiashe/trans/yago310_top10_predictions_val.tsv', sep=\"\\t\", header=0, index_col=0)\n",
    "perplexities = []\n",
    "cnt = 0\n",
    "text = []\n",
    "label = []\n",
    "flag = True\n",
    "for head, relation, pred_tail, true_tail, rank, score in tqdm(data.to_numpy()):\n",
    "    if rank>10:\n",
    "        continue\n",
    "    if cnt%10 == 0:\n",
    "        flag = True\n",
    "    head = \" \".join(head.split(\"_\"))\n",
    "    relation = re.sub( r\"([A-Z])\", r\" \\1\", relation).lower()\n",
    "    # print(relation)\n",
    "    pred_tail = \" \".join(pred_tail.split(\"_\"))\n",
    "    true_tail = \" \".join(true_tail.split(\"_\"))\n",
    "    sentence = \" \".join([head, relation, pred_tail])\n",
    "    if pred_tail == true_tail:\n",
    "        label.append(1)\n",
    "        text.append(sentence)\n",
    "\n",
    "    elif flag:\n",
    "        label.append(0)\n",
    "        text.append(sentence)\n",
    "        flag = False\n",
    "    # text.append(sentence)\n",
    "    # if pred_tail == true_tail:\n",
    "    #     label.append(1)\n",
    "    # else:\n",
    "    #     label.append(0)\n",
    "    # break\n",
    "    cnt+=1\n",
    "\n",
    "    # if cnt>10:\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.DataFrame()\n",
    "# df_data[\"text\"] = text\n",
    "# df_data[\"label\"] = label\n",
    "# df_data[\"text\"].to_csv(\"lp.data.txt\", header=None, index = None)\n",
    "# df_data[\"label\"].to_csv(\"lp.gold.txt\", header=None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame()\n",
    "df_data[\"text\"] = text\n",
    "df_data[\"label\"] = label\n",
    "\n",
    "train, test = train_test_split(df_data, test_size=0.2, random_state=42, stratify=df_data[\"label\"])\n",
    "\n",
    "\n",
    "train[\"text\"].to_csv(\"/nas/home/gujiashe/kb/experiments/train.data.txt\", header=None, index = None)\n",
    "train[\"label\"].to_csv(\"/nas/home/gujiashe/kb/experiments/train.gold.txt\", header=None, index = None)\n",
    "test[\"text\"].to_csv(\"/nas/home/gujiashe/kb/experiments/dev.data.txt\", header=None, index = None)\n",
    "test[\"label\"].to_csv(\"/nas/home/gujiashe/kb/experiments/dev.gold.txt\", header=None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2489\n",
       "0    2488\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred = pd.read_csv('/nas/home/gujiashe/critic/dev1.pred.txt', header=None)\n",
    "gold = pd.read_csv('/nas/home/gujiashe/kb/experiments/train.gold.txt', header=None)\n",
    "gold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     642\n",
       "False    602\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred==gold).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3d935fc17e4411b99d57e667c8b2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fc8d7987774282b1ac8f370179c3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# dataset = Dataset.from_pandas(df_data)\n",
    "# dataset = dataset.train_test_split(test_size=0.2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True)\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import Trainer\n",
    "\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.get(\"labels\")\n",
    "#         # forward pass\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")\n",
    "        \n",
    "#         log_pt = torch.log_softmax(logits, dim=1)\n",
    "#         pt = torch.exp(log_pt)\n",
    "#         log_pt = (1 - pt) ** 2 * log_pt\n",
    "#         labels = labels.to(\"cuda\")\n",
    "#         # log_pt.to(\"cuda\")\n",
    "#         loss = torch.nn.functional.nll_loss(log_pt, labels, None, reduction='mean', ignore_index=-100)\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4976\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 390\n",
      "/nas/home/gujiashe/miniconda3/envs/transformers/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 02:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=390, training_loss=0.6137779040214343, metrics={'train_runtime': 167.8138, 'train_samples_per_second': 148.26, 'train_steps_per_second': 2.324, 'total_flos': 147485889978624.0, 'train_loss': 0.6137779040214343, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "# metric = load_metric('accuracy')\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics = compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1244\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6657101511955261,\n",
       " 'eval_accuracy': 0.5908360128617364,\n",
       " 'eval_runtime': 4.125,\n",
       " 'eval_samples_per_second': 301.574,\n",
       " 'eval_steps_per_second': 4.848,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1031, -2.5563]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = tokenizer(\"Diane_Varsi diedIn Los_Angeles\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# # _model = wrap\n",
    "# model(input_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea87aec47fffddd9cad9f70321a61fb292b3dbedc3074a5520375f904d46251e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('transformers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
