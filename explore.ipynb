{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.evaluate import *\n",
    "from kb.include_all import *\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_file = \"knowbert_wiki_wordnet_model\"\n",
    "cuda_device = 0\n",
    "line = \"banana\\tcolor\\tyellow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knowbert_wiki_wordnet_model\n",
      "<allennlp.common.params.Params object at 0x7f950bcba278>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "archive = load_archive(archive_file, cuda_device)\n",
    "\n",
    "config = archive.config\n",
    "prepare_environment(config)\n",
    "reader_params = config.pop('dataset_reader')\n",
    "print(reader_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_params_c = reader_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate_mentions_cnt:  6777\n",
      "end of p_e_m reading. wall time: 0.7944552024205526  minutes\n",
      "p_e_m_errors:  0\n",
      "incompatible_ent_ids:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scruple/miniconda3/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if reader_params['type'] == 'multitask_reader':\n",
    "    reader_params = reader_params['dataset_readers']['language_modeling']\n",
    "# reader_params['num_workers'] = 0\n",
    "validation_reader_params = {\n",
    "    \"type\": \"food\",\n",
    "    \"tokenizer_and_candidate_generator\": reader_params['base_reader']['tokenizer_and_candidate_generator'].as_dict()\n",
    "}\n",
    "dataset_reader = DatasetReader.from_params(Params(validation_reader_params))\n",
    "vocab = dataset_reader._tokenizer_and_candidate_generator.bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'color', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "instance = dataset_reader.read_food(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 6 with text: \n",
      " \t\t[[CLS], banana, has, color, [MASK], [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t segment_ids: ArrayField with shape: (6,). \n",
      " \t candidates:  \n",
      " \t lm_label_ids: TextField of length 6 with text: \n",
      " \t\t[[CLS], banana, has, color, yellow, [SEP]]\n",
      " \t\tand TokenIndexers : {'lm_labels': 'SingleIdTokenIndexer'} \n",
      " \t mask_indicator: ArrayField with shape: (6,). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:00, 3244.78it/s]\n"
     ]
    }
   ],
   "source": [
    "instances = dataset_reader.read(\"../data/food_compact.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kb.dict_field.DictField at 0x7f91b7264668>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0][\"candidates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/201 [00:00<?, ?it/s]/tmp/pip-req-build-808afw3c/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/tmp/pip-req-build-808afw3c/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "  0%|          | 0/201 [01:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "tensor([[[-5.2374, -4.3524, -5.0789,  ..., -4.1331, -4.9955, -1.6057],\n",
      "         [-6.7754, -5.2830, -6.1453,  ..., -4.6664, -5.9723, -2.9309],\n",
      "         [-2.2188, -0.8701, -3.2900,  ..., -2.8037, -2.6924,  0.1865],\n",
      "         ...,\n",
      "         [-2.1816, -1.0377, -2.0051,  ..., -0.7198, -1.9534,  1.7026],\n",
      "         [-3.2374, -1.9628, -3.4024,  ..., -1.5293, -2.9011,  1.6084],\n",
      "         [-4.3510, -3.5991, -4.4943,  ..., -3.4429, -4.7890,  1.0667]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor([[3, 3, 3,  ..., 3, 3, 3]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ae0c7aa74006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: tensor([[3, 3, 3,  ..., 3, 3, 3]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "model = archive.model\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(\"start\")\n",
    "# metrics = evaluate(model, instances, iterator, cuda_device, \"\")\n",
    "data_iterator = DataIterator.from_params(Params(\n",
    "    {\"type\": \"basic\", \"batch_size\": 1}\n",
    "))\n",
    "data_iterator.index_with(model.vocab)\n",
    "iterator = data_iterator(instances,\n",
    "                            num_epochs=1,\n",
    "                            shuffle=False)\n",
    "logger.info(\"Iterating over dataset\")\n",
    "generator_tqdm = Tqdm.tqdm(iterator, total=data_iterator.get_num_batches(instances))\n",
    "\n",
    "for instance in generator_tqdm:\n",
    "    batch = nn_util.move_to_device(instance, cuda_device)\n",
    "    output_dict = model(**batch)\n",
    "    pooled_output = output_dict.get(\"pooled_output\")\n",
    "    contextual_embeddings = output_dict.get(\"contextual_embeddings\")\n",
    "    prediction_scores, seq_relationship_score = model.pretraining_heads(\n",
    "    contextual_embeddings, pooled_output\n",
    "    )\n",
    "    prediction_scores = prediction_scores.view(-1, prediction_scores.shape[-1])\n",
    "    print(\"================\")\n",
    "    print(prediction_scores)\n",
    "    print(\"================\")\n",
    "    idx = torch.argmin(prediction_scores, dim = 1)\n",
    "    print(line, vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = prediction_scores.view(-1, prediction_scores.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([  101, 21966, 25577, 28313,   101, 23681, 25279,  9243],\n       device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f027ef304cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: tensor([  101, 21966, 25577, 28313,   101, 23681, 25279,  9243],\n       device='cuda:0')"
     ]
    }
   ],
   "source": [
    "idx = torch.argmin(prediction_scores, dim = 1)\n",
    "print(line, vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2word = {}\n",
    "for k, v in vocab.items():\n",
    "    token2word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "##marine\n",
      "tinted\n",
      "##tees\n",
      "[CLS]\n",
      "##dote\n",
      "##ritan\n",
      "##cies\n"
     ]
    }
   ],
   "source": [
    "for i in idx.cpu().numpy():\n",
    "    print(token2word[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32459ed5093ebb7fba2e0a98e8ed8c01f1a4aa667381f7e7c3dd05affc8c7846"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('knowbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
